{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple binary classification problem utilizing convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries. \n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "#os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu0,floatX=float32,optimizer=fast_compile'\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "# \"\"\"\n",
    "# os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu3,floatX=float32,optimizer=fast_compile'\n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "# In case you want to select a graphic card (i the above code i set the 3rd graphic card.) \n",
    "# \"\"\"\n",
    "\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.api.layers import Convolution2D, MaxPooling2D\n",
    "from keras.api.optimizers import SGD\n",
    "from keras.api.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import numpy as np\n",
    "import keras \n",
    "import keras.api.backend as K\n",
    "from keras.api.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from keras import callbacks\n",
    "import glob\n",
    "from PIL import Image\n",
    "from keras.src.utils import plot_model\n",
    "import h5py\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It is good to know the pid of the running code in case you need to stop  or monitor. \n",
    "# print (os.getpid())\n",
    "import keras.api\n",
    "\n",
    "file_open = lambda x,y: glob.glob(os.path.join(x,y))\n",
    "\n",
    "# learning rate schedule. It is helpful when the learning rate can be dynamically set up. We will be using the callback functionality that keras provides. \n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.3\n",
    "    epochs_drop = 30.0\n",
    "    # This function doesn't actually affect the learning rate too much until a higher number of epochs is reached (around 30)\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    #print(\"Learning rate:\", lrate)\n",
    "    return lrate\n",
    "\n",
    "# The following function will be used to give a number of the parameters in our model. Useful when we need to get an estimate of what size of dataset we have to use.  \n",
    "def size(model): \n",
    "    return sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "def createmodel(img_rows, img_cols, optimizer, loss):\n",
    "    # This is a Sequential model. Graph models can be used in order to create more complex networks. \n",
    "    # Teaching Points:\n",
    "    # 1. Here we utilize the adam optimization algorithm. In order to use the SGD algorithm one could replace the {adam=keras.optimizers.Adadelta(lr=0)} line with  {sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)} make sure you import the correct optimizer from keras. \n",
    "    # 2. This is a binary classification problem so make sure that the correct activation loss function combination is used. For such a problem the sigmoid activation function with the binary cross entropy loss is a good option\n",
    "    # 3. Since this is a binary problem use   model.add(Dense(1)) NOT 2...\n",
    "    # 4. For multi class model this code can be easily modified by selecting the softmax as activation function and the categorical cross entropy as loss \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, padding='same',input_shape=(img_rows, img_cols, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(16, 5, 5, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(32, 3, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 5, 5, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(128, 3, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(32, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # learning schedule callback\n",
    "    \n",
    "    # Original code had the variable named \"adam\", but the selected optimizer was adadelta (they are similar optimizers but different slightly)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "def shuffle(X, y):\n",
    "    perm = np.random.permutation(len(y))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    #print(\"shuffle() new shape for x: \", np.shape(X))\n",
    "    return X, y\n",
    "\n",
    "def read_data(image):\n",
    "    \"opens image and converts it to a m*n matrix\" \n",
    "    image = Image.open(image)\n",
    "    image = image.getdata()\n",
    "\n",
    "    image = np.array(image)\n",
    "    return image.reshape(-1)\n",
    "\n",
    "def createTrainTestValset(image_dir1, image_dir2):\n",
    "    Class1_images = file_open(image_dir1,\"*.jpg\")\n",
    "    Class2_images = file_open(image_dir2,\"*.jpg\")\n",
    "\n",
    "    # Read all the files, and create numpy arrays. \n",
    "    Class1_set = np.array([read_data(image) for image in Class1_images])\n",
    "    Class2_set = np.array([read_data(image) for image in Class2_images])\n",
    "    X = np.vstack((Class1_set, Class2_set))\n",
    "    \n",
    "    X = X / 255.0\n",
    "\n",
    "    yclass1 = np.zeros((np.shape(Class1_set)[0]))\n",
    "    yclass2 = np.ones((np.shape(Class2_set)[0]))\n",
    "    \n",
    "    y = np.concatenate((yclass1, yclass2))\n",
    "    \n",
    "    X,y = shuffle(X, y)\n",
    "\n",
    "    # print(\"X shape:\", np.shape(X)) \n",
    "    # print(\"X max:\", np.max(X))\n",
    "    # print(\"Y shape:\", np.shape(y)) \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val \n",
    "\n",
    "# Read the images; and split them in three different sets. \n",
    "def trainandpredict(X_train, y_train, X_val, y_val, optimizer, loss, lrate, batch_size=64, nb_epoch=5):\n",
    "    \"\"\"\n",
    "    Train the model using some of the inputs, predict the remainder of the inputs using the fitted model and print the report.\n",
    "\n",
    "    :param optimizer: a keras.optimizers object that the model will recieve during compilation (could also be a string)\n",
    "    :param loss: a keras.losses object that the model will recieve during compilation (could also be a string)\n",
    "    :param lrate: a LearningRateScheduler that the model will consider during fitting\n",
    "    :param checkpoint_file_name: name of the file to save weights to. These values are used and altered during fitting, so \n",
    "      be sure to use the correct file for the correct model. Trying to use one weights file for a different model will most likely result\n",
    "      in an error. Have different weights files for different versions of the model.\n",
    "    :param batch_size: batch size used during fitting\n",
    "    :param nb_epoch: number of epochs run during fitting\n",
    "    \"\"\"\n",
    "    img_rows = 32\n",
    "    img_cols = 32\n",
    "    \n",
    "    modeleval = createmodel(img_rows, img_cols, optimizer, loss)\n",
    "\n",
    "    X_train = X_train.reshape(\n",
    "      -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "      img_rows,  # first image dimension (vertical)\n",
    "      img_cols,  # second image dimension (horizontal)\n",
    "      1,   # 1 color channel, since images are only black and white\n",
    "    )\n",
    "    X_val = X_val.reshape(\n",
    "      -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "      img_rows,  # first image dimension (vertical)\n",
    "      img_cols,  # second image dimension (horizontal)\n",
    "      1,   # 1 color channel, since images are only black and white\n",
    "    )\n",
    "\n",
    "    # filepath = checkpoint_file_name + '.weights.h5'\n",
    "\n",
    "    # Callbacks (Don't need them for this project, but very useful for saving weights for later use)\n",
    "    # best_model = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "\n",
    "    # try:\n",
    "    #   modeleval.load_weights(filepath)\n",
    "    # except FileNotFoundError:\n",
    "    #   print(f\"Could not find file: {filepath}, assuming this is the first time with this model and will create a new file\")\n",
    "    # except ValueError as e:\n",
    "    #   print(e)\n",
    "    #   print(\"!!!!!!!ValueError detected, assuming this is a new model and a filepath for a different model's weights was inputted, consider a new weights file\")\n",
    "    #   sys.exit()\n",
    "\n",
    "    #modeleval.fit(X_train, y_train,batch_size=batch_size,epochs=nb_epoch,validation_split=0.1,callbacks=[best_model,lrate],shuffle=True, verbose=0)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    modeleval.fit(X_train, y_train,batch_size=batch_size,epochs=nb_epoch,validation_split=0.1,callbacks=[lrate],shuffle=True, verbose=0)\n",
    "    \n",
    "\n",
    "    print(\"Total time to fit:\", time.time() - start)\n",
    "\n",
    "    # Some evaluation Just the basic stuff... \n",
    "    #print (\"Dir:\", dir(modeleval))\n",
    "    Y_cv_pred = modeleval.predict(X_val, batch_size = 32)\n",
    "    roc = roc_auc_score(y_val, Y_cv_pred)\n",
    "    print(\"ROC:\", roc)\n",
    "    #print (\"Y_cv_pred:\", Y_cv_pred)\n",
    "\n",
    "    Y_cv_pred[Y_cv_pred>=.5]=1\n",
    "    Y_cv_pred[Y_cv_pred<.5]=0\n",
    "    \n",
    "    target_names = ['class 0', 'class 1']\n",
    "    # Default notebook output size might not show all information from the result, make sure to expand it or change a setting when viewing\n",
    "    print(classification_report(y_val, Y_cv_pred, target_names=target_names,digits=4))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.optimizers.adadelta.Adadelta'>\n",
      "<class 'keras.src.losses.losses.BinaryCrossentropy'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Data\\VSCodeProjects\\MachineLearning\\ProjectCode\\virtualenv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to fit: 11.168266773223877\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "ROC: 0.6839190137755549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0     0.3582    0.8969    0.5120       524\n",
      "     class 1     0.8744    0.3087    0.4563      1218\n",
      "\n",
      "    accuracy                         0.4856      1742\n",
      "   macro avg     0.6163    0.6028    0.4841      1742\n",
      "weighted avg     0.7191    0.4856    0.4731      1742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Some of the optimizers provided by keras can take in variations of the\n",
    "    # keras.optimizers.schedules.LearningRateSchedule objects like\n",
    "    # keras.optimizers.schedules.ExponentialDecay for example, something to consider.\n",
    "    # This is NOT the same type of object as the lrate variable (keras.callbacks.LearningRateScheduler),\n",
    "    # which might be the source of the UserWarning when running, unsure of how to fix this.\n",
    "\n",
    "    CurrentDir = os.getcwd()\n",
    "    image_dir1 = os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\", \"negative_images\"))\n",
    "    image_dir2 = os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\", \"positive_images\"))\n",
    "\n",
    "    X_train, y_train, X_val, y_val = createTrainTestValset(image_dir1, image_dir2)\n",
    "\n",
    "\n",
    "    optimizers = [keras.optimizers.Adadelta, keras.optimizers.Adafactor, keras.optimizers.Adagrad, keras.optimizers.Adam, keras.optimizers.Adamax,\n",
    "                  keras.optimizers.AdamW, keras.optimizers.Ftrl, keras.optimizers.Lion, keras.optimizers.Nadam, keras.optimizers.RMSprop, \n",
    "                  keras.optimizers.SGD]\n",
    "    \n",
    "    losses = [keras.losses.BinaryCrossentropy, keras.losses.BinaryFocalCrossentropy, keras.losses.SquaredHinge, keras.losses.Dice, \n",
    "              keras.losses.Huber, keras.losses.KLDivergence, keras.losses.MeanAbsoluteError]\n",
    "\n",
    "\n",
    "    for optimizer in optimizers: \n",
    "        for loss in losses: \n",
    "            print(optimizer)\n",
    "            print(loss)\n",
    "            trainandpredict(X_train, y_train, X_val, y_val,\n",
    "                            optimizer=optimizer(learning_rate=0.0), \n",
    "                            loss=loss,\n",
    "                            lrate=LearningRateScheduler(step_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

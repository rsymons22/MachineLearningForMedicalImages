{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple binary classification problem utilizing convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries. \n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "#os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu0,floatX=float32,optimizer=fast_compile'\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "# \"\"\"\n",
    "# os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu3,floatX=float32,optimizer=fast_compile'\n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "# In case you want to select a graphic card (i the above code i set the 3rd graphic card.) \n",
    "# \"\"\"\n",
    "\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.api.layers import Convolution2D, MaxPooling2D\n",
    "from keras.api.optimizers import SGD\n",
    "from keras.api.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import numpy as np\n",
    "import keras \n",
    "import keras.api.backend as K\n",
    "from keras.api.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from keras import callbacks\n",
    "import glob\n",
    "from PIL import Image\n",
    "from keras.src.utils import plot_model\n",
    "import h5py\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It is good to know the pid of the running code in case you need to stop  or monitor. \n",
    "# print (os.getpid())\n",
    "import keras.api\n",
    "\n",
    "file_open = lambda x,y: glob.glob(os.path.join(x,y))\n",
    "\n",
    "# learning rate schedule. It is helpful when the learning rate can be dynamically set up. We will be using the callback functionality that keras provides. \n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.3\n",
    "    epochs_drop = 30.0\n",
    "    # This function doesn't actually affect the learning rate too much until a higher number of epochs is reached (around 30)\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    #print(\"Learning rate:\", lrate)\n",
    "    return lrate\n",
    "\n",
    "# The following function will be used to give a number of the parameters in our model. Useful when we need to get an estimate of what size of dataset we have to use.  \n",
    "def size(model): \n",
    "    return sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "def createmodel(img_rows, img_cols, optimizer, loss, activation):\n",
    "    \"\"\"\n",
    "    Creates a keras model\n",
    "\n",
    "    :param img_rows: vertical height of the image in pixels\n",
    "    :param img_cols: horizontal width of the image in pixels\n",
    "    :param optimizer: a keras.optimizers object that the model will recieve during compilation (could also be a string)\n",
    "    :param loss: a keras.losses object that the model will recieve during compilation (could also be a string)\n",
    "    :param activation: a keras.layers.activation object that will be used in the model's construction\n",
    "    :return: a sequential keras model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # This is a Sequential model. Graph models can be used in order to create more complex networks. \n",
    "    # Teaching Points:\n",
    "    # 1. Here we utilize the adam optimization algorithm. In order to use the SGD algorithm one could replace the {adam=keras.optimizers.Adadelta(lr=0)} line with  {sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)} make sure you import the correct optimizer from keras. \n",
    "    # 2. This is a binary classification problem so make sure that the correct activation loss function combination is used. For such a problem the sigmoid activation function with the binary cross entropy loss is a good option\n",
    "    # 3. Since this is a binary problem use   model.add(Dense(1)) NOT 2...\n",
    "    # 4. For multi class model this code can be easily modified by selecting the softmax as activation function and the categorical cross entropy as loss \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, padding='same',input_shape=(img_rows, img_cols, 1)))\n",
    "    model.add(activation)\n",
    "    model.add(Convolution2D(16, 5, 5, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(activation)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(32, 3, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(activation)\n",
    "    model.add(Convolution2D(64, 5, 5, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(activation)\n",
    "    model.add(Convolution2D(64, 3, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(activation)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(128, 3, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(activation)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(activation)\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(32, kernel_initializer='he_normal'))\n",
    "    model.add(activation)\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # learning schedule callback\n",
    "    \n",
    "    # Original code had the variable named \"adam\", but the selected optimizer was adadelta (they are similar optimizers but different slightly)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Shuffles the given arrays\n",
    "\n",
    "    :param X: images\n",
    "    :param y: labels\n",
    "    :return: tuple (X, y) that has been shuffled\n",
    "    \"\"\"\n",
    "    perm = np.random.permutation(len(y))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    return X, y\n",
    "\n",
    "def read_data(image):\n",
    "    \"\"\"\n",
    "    Opens image and converts it to a m*n matrix\n",
    "\n",
    "    :param image: image file\n",
    "    :return: matrix of pixels\n",
    "    \"\"\"\n",
    "    image = Image.open(image)\n",
    "    image = image.getdata()\n",
    "\n",
    "    image = np.array(image)\n",
    "    return image.reshape(-1)\n",
    "\n",
    "def createTrainTestValset(image_dir1, image_dir2):\n",
    "    \"\"\"\n",
    "    Creates the train and test values from the given image directories\n",
    "\n",
    "    :param image_dir1: directory of jpg images for class 1\n",
    "    :param image_dir2: directory of jpg images for class 2\n",
    "    :return: training and testing images/labels\n",
    "    \"\"\"\n",
    "    Class1_images = file_open(image_dir1,\"*.jpg\")\n",
    "    Class2_images = file_open(image_dir2,\"*.jpg\")\n",
    "\n",
    "    # Read all the files, and create numpy arrays. \n",
    "    Class1_set = np.array([read_data(image) for image in Class1_images])\n",
    "    Class2_set = np.array([read_data(image) for image in Class2_images])\n",
    "    X = np.vstack((Class1_set, Class2_set))\n",
    "    \n",
    "    X = X / 255.0\n",
    "\n",
    "    yclass1 = np.zeros((np.shape(Class1_set)[0]))\n",
    "    yclass2 = np.ones((np.shape(Class2_set)[0]))\n",
    "    \n",
    "    y = np.concatenate((yclass1, yclass2))\n",
    "    \n",
    "    X,y = shuffle(X, y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val \n",
    "\n",
    "# Read the images; and split them in three different sets. \n",
    "def trainandpredict(X_train, y_train, X_val, y_val, optimizer, loss, lrate, activation=Activation(keras.activations.relu), batch_size=64, nb_epoch=5):\n",
    "    \"\"\"\n",
    "    Train the model using some of the inputs, predict the remainder of the inputs using the fitted model and print the report.\n",
    "\n",
    "    :param x_train: training images\n",
    "    :param y_train: training labels\n",
    "    :param X_val: test images\n",
    "    :param y_val: test labels\n",
    "    :param optimizer: a keras.optimizers object that the model will recieve during compilation (could also be a string)\n",
    "    :param loss: a keras.losses object that the model will recieve during compilation (could also be a string)\n",
    "    :param lrate: a LearningRateScheduler that the model will consider during fitting\n",
    "    :param activation: a keras.layers.activation object that will be used in the model's construction\n",
    "    :param batch_size: batch size used during fitting\n",
    "    :param nb_epoch: number of epochs run during fitting\n",
    "    \"\"\"\n",
    "    img_rows = 32\n",
    "    img_cols = 32\n",
    "    \n",
    "    modeleval = createmodel(img_rows, img_cols, optimizer, loss, activation)\n",
    "\n",
    "    X_train = X_train.reshape(\n",
    "      -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "      img_rows,  # first image dimension (vertical)\n",
    "      img_cols,  # second image dimension (horizontal)\n",
    "      1,   # 1 color channel, since images are only black and white\n",
    "    )\n",
    "    X_val = X_val.reshape(\n",
    "      -1,  # number of samples, -1 makes it so that this number is determined automatically\n",
    "      img_rows,  # first image dimension (vertical)\n",
    "      img_cols,  # second image dimension (horizontal)\n",
    "      1,   # 1 color channel, since images are only black and white\n",
    "    )\n",
    "\n",
    "    # filepath = 'Final.weights.h5'\n",
    "\n",
    "    # Callbacks (Don't need them for this project, but very useful for saving weights for later use)\n",
    "    # best_model = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "\n",
    "    # try:\n",
    "    #   modeleval.load_weights(filepath)\n",
    "    # except FileNotFoundError:\n",
    "    #   print(f\"Could not find file: {filepath}, assuming this is the first time with this model and will create a new file\")\n",
    "    # except ValueError as e:\n",
    "    #   print(e)\n",
    "    #   print(\"!!!!!!!ValueError detected, assuming this is a new model and a filepath for a different model's weights was inputted, consider a new weights file\")\n",
    "    #   sys.exit()\n",
    "\n",
    "    # modeleval.fit(X_train, y_train,batch_size=batch_size,epochs=nb_epoch,validation_split=0.1,callbacks=[best_model,lrate],shuffle=True, verbose=0)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    modeleval.fit(X_train, y_train,batch_size=batch_size,epochs=nb_epoch,validation_split=0.1,callbacks=[lrate],shuffle=True, verbose=0)\n",
    "\n",
    "    print(\"Total time to fit:\", time.time() - start)\n",
    "\n",
    "    # Some evaluation Just the basic stuff... \n",
    "    Y_cv_pred = modeleval.predict(X_val, batch_size = 32)\n",
    "    roc = roc_auc_score(y_val, Y_cv_pred)\n",
    "    print(\"ROC:\", roc)\n",
    "\n",
    "    Y_cv_pred[Y_cv_pred>=.5]=1\n",
    "    Y_cv_pred[Y_cv_pred<.5]=0\n",
    "    \n",
    "    target_names = ['class 0', 'class 1']\n",
    "    # Default notebook output size might not show all information from the result, make sure to expand it or change a setting when viewing\n",
    "    print(classification_report(y_val, Y_cv_pred, target_names=target_names,digits=4))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.optimizers.adadelta.Adadelta'>\n",
      "<class 'keras.src.losses.losses.BinaryCrossentropy'>\n",
      "Total time to fit: 10.79953932762146\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "ROC: 0.664991963928357\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0     0.4637    0.4487    0.4560       526\n",
      "     class 1     0.7648    0.7755    0.7701      1216\n",
      "\n",
      "    accuracy                         0.6768      1742\n",
      "   macro avg     0.6142    0.6121    0.6131      1742\n",
      "weighted avg     0.6739    0.6768    0.6753      1742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    CurrentDir = os.getcwd()\n",
    "    image_dir1 = os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\", \"negative_images\"))\n",
    "    image_dir2 = os.path.abspath(os.path.join(os.path.abspath(os.path.join(CurrentDir, os.pardir)), \"Data\", \"positive_images\"))\n",
    "\n",
    "    X_train, y_train, X_val, y_val = createTrainTestValset(image_dir1, image_dir2)\n",
    "\n",
    "    # Part 1\n",
    "\n",
    "    optimizers = [keras.optimizers.Adadelta, keras.optimizers.Adafactor, keras.optimizers.Adagrad, keras.optimizers.Adam, keras.optimizers.Adamax,\n",
    "                  keras.optimizers.AdamW, keras.optimizers.Ftrl, keras.optimizers.Lion, keras.optimizers.Nadam, keras.optimizers.RMSprop, \n",
    "                  keras.optimizers.SGD]\n",
    "    \n",
    "    losses = [keras.losses.BinaryCrossentropy, keras.losses.BinaryFocalCrossentropy, keras.losses.SquaredHinge, keras.losses.Dice, \n",
    "              keras.losses.Huber, keras.losses.KLDivergence, keras.losses.MeanAbsoluteError]\n",
    "\n",
    "    # Simple grid search\n",
    "    for optimizer in optimizers: \n",
    "        for loss in losses: \n",
    "            print(optimizer)\n",
    "            print(loss)\n",
    "            trainandpredict(X_train, y_train, X_val, y_val,\n",
    "                            optimizer=optimizer(learning_rate=0.0), \n",
    "                            loss=loss,\n",
    "                            # This step_decay function doesn't actually change the learning rate until\n",
    "                            # the number of epochs reaches about 30, a number of epochs we are not testing.\n",
    "                            # For all intents and purposes the learning rate is essentially constant at 0.01.\n",
    "                            lrate=LearningRateScheduler(step_decay))\n",
    "            \n",
    "\n",
    "    # Part 2 \n",
    "\n",
    "    # Selected manually from looking at part 1 results\n",
    "    best_combinations = [(keras.optimizers.Nadam, keras.losses.Huber), (keras.optimizers.RMSprop, keras.losses.Huber),\n",
    "                         (keras.optimizers.AdamW, keras.losses.Huber), (keras.optimizers.Nadam, keras.losses.SquaredHinge),\n",
    "                         (keras.optimizers.Adamax, keras.losses.BinaryCrossentropy)]\n",
    "\n",
    "    # # Testing activation functions\n",
    "    activations = [Activation(keras.activations.relu), Activation(keras.activations.selu), Activation(keras.activations.sigmoid)]\n",
    "\n",
    "    for comb in best_combinations:\n",
    "        for a in activations:\n",
    "            print(\"optimizer:\", comb[0])\n",
    "            print(\"loss:\", comb[1])\n",
    "            print(\"act:\", a.activation)\n",
    "            trainandpredict(X_train, y_train, X_val, y_val,\n",
    "                    optimizer=comb[0](learning_rate=0.0), \n",
    "                    loss=comb[1],\n",
    "                    lrate=LearningRateScheduler(step_decay),\n",
    "                    activation=a)\n",
    "            \n",
    "    # Testing batch sizes\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    for comb in best_combinations:\n",
    "        for b in batch_sizes:\n",
    "            print(\"optimizer:\", comb[0])\n",
    "            print(\"loss:\", comb[1])\n",
    "            print(\"batch_size:\", b)\n",
    "            trainandpredict(X_train, y_train, X_val, y_val,\n",
    "                    optimizer=comb[0](learning_rate=0.0), \n",
    "                    loss=comb[1],\n",
    "                    lrate=LearningRateScheduler(step_decay),\n",
    "                    batch_size=b)\n",
    "            \n",
    "    # Testing epochs\n",
    "    epochs = [5, 10, 15]\n",
    "    for comb in best_combinations:\n",
    "        for e in epochs:\n",
    "            print(\"optimizer:\", comb[0])\n",
    "            print(\"loss:\", comb[1])\n",
    "            print(\"epochs:\", e)\n",
    "            trainandpredict(X_train, y_train, X_val, y_val,\n",
    "                    optimizer=comb[0](learning_rate=0.0), \n",
    "                    loss=comb[1],\n",
    "                    lrate=LearningRateScheduler(step_decay),\n",
    "                    nb_epoch=e)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
